{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitd28d5bfd303b43519c8e7d8f134c3bbd",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "pygame 2.0.0.dev6 (SDL 2.0.10, python 3.8.3)\nHello from the pygame community. https://www.pygame.org/contribute.html\n2.2.0\n"
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class snek:\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.rewards = None\n",
    "        self.action_space.n = 4\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.cstate = None\n",
    "        self.dis = None\n",
    "        self.snake = None\n",
    "        self.head = None\n",
    "        self.food = None\n",
    "        self.scolor = (0, 0, 255)\n",
    "        self.fcolor = (255, 0, 0)\n",
    "        self.len = 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.snake = []\n",
    "        state = np.zeros((self.size, self.size))\n",
    "        size = self.size\n",
    "        x = random.randint(0, size - 1)\n",
    "        y = random.randint(0, size - 1)\n",
    "        foodx = random.randint(0, size - 1)\n",
    "        foody = random.randint(0, size - 1) \n",
    "        self.food = (foodx, foody)\n",
    "        self.head = (x, y)\n",
    "        self.snake.append((x, y))\n",
    "        state[x][y] = 1\n",
    "        state[foodx][foody] = -1\n",
    "        self.cstate = state\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Left = 0 Right = 1 Up = 2 Down = 3\n",
    "        done = False\n",
    "        reward = 0\n",
    "        dx, dy = 0, 0\n",
    "        if action == 0:\n",
    "            dx, dy = -1, 0\n",
    "        elif action == 1:\n",
    "            dx, dy = 1, 0\n",
    "        elif action == 2:\n",
    "            dx, dy = 0, -1\n",
    "        elif action == 3:\n",
    "            dx, dy = 0, 1\n",
    "        \n",
    "        \n",
    "        x, y = self.head\n",
    "        x += dy\n",
    "        y += dx\n",
    "        if x>=self.size or x<0 or y>=self.size or y<0:\n",
    "            done = True\n",
    "            print('Reached end')\n",
    "        \n",
    "        for x in self.snake[:-1]:\n",
    "            if x == self.head:\n",
    "                done = True\n",
    "                print(\"ate itself\")\n",
    "        \n",
    "        state = np.zeros((self.size, self.size))\n",
    "        \n",
    "        if not done:\n",
    "            self.head = (x,y)\n",
    "            self.snake.append(self.head)\n",
    "\n",
    "            if self.head == self.food:\n",
    "                reward = 1\n",
    "                self.len += 1\n",
    "                self.food = (random.randint(0, self.size - 1), random.randint(0, self.size - 1))\n",
    "                print('Food')\n",
    "\n",
    "            elif self.len < len(self.snake):\n",
    "                del self.snake[0]\n",
    "            \n",
    "            \n",
    "            for x in self.snake:\n",
    "                state[x[0]][x[1]] = 1\n",
    "            state[self.food[0]][self.food[1]] = -1\n",
    "            self.cstate = state\n",
    "\n",
    "        if done == True:\n",
    "            reward = -10\n",
    "        return state, reward, done\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL:\n",
    "\n",
    "    def __init__(self, env, model, epsilon = 1, decay_rate = 0.996, min_epsilon = 0.1, discount_factor = 0.95, ermax = 500000, name = 'Default'):\n",
    "        \"\"\"\n",
    "        Function which initializes all the values and parameters for the Deep Q Learning\n",
    "\n",
    "        Input:\n",
    "        env - Open AI Environment\n",
    "        model - Q learning model specific to the environment\n",
    "        epsilon - The rate of being greedy (Default = 1)\n",
    "        decay_rate - Epsilon decay rate (Default = 0.99)\n",
    "        min_epsilon - Minimum value of epsilon (Default = 0.1)\n",
    "        discount_factor - Gamma value for RL (Default = 1)\n",
    "        name - Model's Name (Default = 'Default)\n",
    "\n",
    "        Returns:\n",
    "        DQL Object\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.er = []\n",
    "        self.ermax = ermax\n",
    "        self.rewards = []\n",
    "        self.mse = []\n",
    "        self.batch_size = 0\n",
    "\n",
    "    def predict(self, state):\n",
    "        '''\n",
    "        Helper function to reshape and predict\n",
    "        '''\n",
    "        return self.model.predict(state.reshape(1, -1))\n",
    "\n",
    "    def epsilon_policy(self, state, rand = False):\n",
    "        '''\n",
    "        Function which finds the action probabilities based on epsilon policy given a specific state\n",
    "\n",
    "        Input:\n",
    "        state - The current state of the environment\n",
    "\n",
    "        Returns:\n",
    "        Probabilities of all the actions\n",
    "        ''' \n",
    "\n",
    "        if random.random() <= self.epsilon or rand == True:\n",
    "            return random.randrange(self.env.action_space.n)\n",
    "        \n",
    "        else:\n",
    "            return np.argmax(self.predict(state)[0])\n",
    "\n",
    "    def plot(self, i, tot_reward, tot_loss):\n",
    "        '''\n",
    "        Function to plot mse and reward by epiode basis\n",
    "\n",
    "        Input:\n",
    "        i - Current episode number\n",
    "        tot_reward - Total reward for current episode\n",
    "        tot_loss - Total loss for current episode\n",
    "\n",
    "        '''\n",
    "        clear_output(wait = True)\n",
    "        fig, ax = plt.subplots(1, 2, figsize = (12, 5))\n",
    "        fig.suptitle(\"Episode : {} Reward: {} Loss: {} Epsilon: {}\".format(i, tot_reward, round(tot_loss, 3), round(self.epsilon, 3)))\n",
    "        ax[0].set_xlabel('Episodes')\n",
    "        ax[0].set_ylabel('Rewards')\n",
    "        ax[1].set_xlabel('Episodes')\n",
    "        ax[1].set_ylabel('MSE')\n",
    "        \n",
    "        ax[1].plot(self.mse)\n",
    "        ax[0].plot(self.rewards, label = 'Reward')\n",
    "        ax[0].legend()\n",
    "        ax[0].grid()\n",
    "        ax[1].grid()\n",
    "        plt.show()\n",
    "\n",
    "    def prepopulate_er(self):\n",
    "        '''\n",
    "        Function to prepopulate the experience replay by batch size\n",
    "        '''\n",
    "\n",
    "        state = self.env.reset()\n",
    "        action = self.epsilon_policy(state, True)\n",
    "        states = []\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            states.append(state)\n",
    "            \n",
    "            if i != 0 and i % 4 == 0:\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                self.er.append((states, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "            \n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    \n",
    "    def update_network(self):\n",
    "        '''\n",
    "        Function which updates the network to the target q values\n",
    "        '''\n",
    "\n",
    "        samples = random.sample(self.er, self.batch_size)\n",
    "        States = np.array([s[0] for s in samples])\n",
    "        Next_States = np.array([s[3] for s in samples])\n",
    "        \n",
    "        target = self.model.predict(States)\n",
    "        target_next = self.model.predict(Next_States)\n",
    "\n",
    "        for j,s in enumerate(samples):\n",
    "            state, action, reward, next_state, done = s\n",
    "            if done:\n",
    "                target[j][action] = reward\n",
    "            else:\n",
    "                target[j][action] = reward + (self.discount_factor * np.amax(target_next[j]))\n",
    "        \n",
    "        hist = self.model.fit(States, target, batch_size = self.batch_size, verbose = False)\n",
    "        return round(hist.history['loss'][0], 3)\n",
    "\n",
    "    def learn(self, noe, batch_size):\n",
    "        '''\n",
    "        Function which trains the agent\n",
    "\n",
    "        Input:\n",
    "        noe - No of episodes to train\n",
    "        batch_size - Experience replay batch train size\n",
    "        '''\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.prepopulate_er()\n",
    "\n",
    "        for i in range(noe):\n",
    "            tot_reward = 0\n",
    "            tot_loss = 0\n",
    "            length = 0\n",
    "            self.epsilon = (max(self.min_epsilon, self.epsilon*self.decay_rate))\n",
    "            state = self.env.reset()\n",
    "            pygame.init()\n",
    "            dis = pygame.display.set_mode((self.env.size *10, self.env.size *10))\n",
    "            pygame.display.set_caption(\"AI Learns to play Snake\")\n",
    "            icon = pygame.image.load('/home/veeraraghavan/Desktop/DQL-Snake/Images/snake.png')\n",
    "            pygame.display.set_icon(icon)\n",
    "            gameover = False\n",
    "            state = env.reset()\n",
    "            while gameover != True:\n",
    "                dis.fill((0, 0, 0))\n",
    "                action = self.epsilon_policy(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                if done:\n",
    "                    gameover = True\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        gameover = True\n",
    "                for square in env.snake:\n",
    "                    pygame.draw.rect(dis, env.scolor, [square[1] * 10, square[0] * 10, 10, 10])\n",
    "                pygame.draw.rect(dis, env.fcolor, [env.food[1] * 10, env.food[0] * 10, 10, 10])\n",
    "                pygame.display.update()\n",
    "                #time.sleep(1)\n",
    "            pygame.quit()   \n",
    "\n",
    "            tot_loss/=length\n",
    "            self.rewards.append(tot_reward)\n",
    "            self.mse.append(tot_loss)\n",
    "            self.plot(i, tot_reward, tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = snek(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}